# Related Work {#sec:related}

In the following, I will outline the background of this thesis in a brief review of related literature and projects, covering this work’s research goals of _feasibility_, _usability_, and _interoperability_ of a decentralized annotation system. As opposed to the later @sec:annotation, in this chapter I attempt to draw a cohesive, non-judgmental picture of related work. The discussed works are _intertwingled_[^intertwingled] to a certain degree, but I will try to cover them in chronological order.

This includes development of hypertext systems and the world-wide web, digital libraries, and digital annotation in @sec:related:hypertext and the contemporary development of real-time collaborative web applications in @sec:related:collaboration. In @sec:related:ld-dh, I will highlight the prospects of Linked Data and its impact on the Digital Humanities, followed by a brief discussion of fundamental peer-to-peer technology and file-sharing systems in @sec:related:p2p. Finally, in @sec:related:local-first I connect the earlier aspects of real-time collaboration, hypertext, and decentralization, in order to outline a recent development known as _local-first applications_.

[^intertwingled]: \citeauthor{nelson1987b} coined this term in his 1974 book, "\citetitle{nelson1987b}", in order to describe the complex relations of human knowledge.

## Hypertext and Annotation {#sec:related:hypertext}
 
Influenced by early works on interactive knowledge processing such as Vannevar Bush's _Memex_, the concept of hypertext has initially been shaped by \citeauthor{nelson1965} in \citeyear{nelson1965}:

> _[Hypertext] mean[s] a body of written or pictorial material interconnected in such a complex way that it could not conveniently be presented or represented on paper. It may contain sunmmries, or maps of its contents and their interrelations; it may contain annotations, additions and footnotes from scholars who have examined it._ [@nelson1965]

Particularly relations between documents known as _hyperlinks_ became popular among hypertext pioneers, enabling writers and readers alike to explore text in a non-sequential way. In "\citetitle{nelson1993}"---a print publication that has actually been designed as hypertext---\citeauthor{nelson1993} sketches a system called *Xanadu* that goes even further and processes chunks of digital texts into entire documents, backed by a network of servers for managing these units (@fig:xanalogical-storage). Called _transclusion_, the resulting documents would be composed of the aforementioned chunks, and documents as well as chunks themselves were to be stored with each their editing history, something \citeauthor{nelson1993} calls "an evolving ongoing braid" [@nelson1993, 2/14f]. Furthermore, document servers would manage documents by storing their entire history on disk.

![Documents created by transclusion of units from xanalogical storage in Xanadu [@nelson1993, 0/6]. These units can be standalone pieces of text or digital media and have links inbetween them.](figures/nelson-xanalogical-storage.png){#fig:xanalogical-storage short-caption="Documents created by transclusion of units from xanalogical storage"}

While there had been progress on a multitude of hypertext systems[^hypertext-timeline], the World Wide Web (WWW), invented by Tim Berners-Lee, became what we today commonly refer to as the _web_ [@berners-lee1989a]. Originally designed to distributedly serve documents within the CERN institution, \citeauthor{berners-lee1989a} subsequently created a dedicated protocol, Hypertext Transfer Protocol [HTTP, @fielding1999], and a specialized, hypertext-capable markup language, the Hypertext Markup Language [HTML, @berners-lee1995].

![Analysis of various types of annotation curated by @marshall1997.](figures/marshall-annotation.png){#fig:marshall-annotation short-caption="Analysis of various types of annotation"}

With the advent of world-scale hypertext systems as well as technological advances in the digitization of physical documents, the first digital libraries emerged. One such library, the Perseus Digital Library, has been founded by the Perseus Project and spans a vast collection of structured texts, translations, and dictionaries [@smith2000]. Not limited to the physical appearance of resources, digital libraries can utilize tools for Natural Language Processing (NLP) and provide readers with contextual information. Furthermore, these digital collections of text suggest opportunities for digital annotation, since bodies of annotations can be virtually related to the source and thus create a publicly-accessible collection of derived semantic knowledge. \citeauthor{marshall1997} predicted such prospects in her pioneering work and analyzed over 150 pre-owned students' books that have been shared and annotated by various generations of students [@marshall1997]. She categorized the observed annotations—e.g., underlining, high-level highlighting, marginal notes—into six functional categories—e.g., aiding memory and interpretation—pictured in @fig:marshall-annotation. In a follow-up publication, \citeauthor{marshall1998} gives an outlook of how these observations could be applied to annotation of hypertext.

Resources on the web are usually identified by Uniform Resource Locators (URL); for example, the URL `https://www.ccc.de/de/support` refers to a resource of path `/de/support` that resides on the host behind `www.ccc.de`. URLs, however, do not support _canonical referencing_, i.e., identifying resources independently across systems. This can hinder workflows in digital libraries and academia, which demand elaborate referencing mechanisms.

Alternative identification schemes such as Digital Object Identifiers[^doi] (DOIs) allow to establish end-to-end persistent referencing. Thereby, references to particular versions of a document with each their own DOI can be resolved independently of network hosts. The Canonical Text Service (CTS) system continues even further and provides a scheme for referencing individual passages of text [@koentges2020]. Originally developed for use with classic texts, CTS allows to reference passages of a work independently of its medium. Other distributed systems, such as Git and IPFS, commonly leverage content addressing mechanisms as I will detail later in this chapter.

[^hypertext-timeline]: Jakob Voß prepared an interactive visualization of the timeline of hypertext, backend by structured knowledge on Wikidata: <http://jakobvoss.de/hypertext-timeline/>.
[^doi]: <https://www.doi.org/>.

## Digital Real-Time Collaboration {#sec:related:collaboration}

Further advances in digital technology and, particularly, in real-time data communication via mobile networks and increased internet bandwidth, offereing further prospects of real-time collaborative work done digitally. In \citeyear{dourish1992}, \citeauthor{dourish1992} already derived requirements for shared digital workspaces from studies on remote work. Complementing the necessity of real-time _communication_ for participants (such as a voice call), they state that applications realizing real-time _collaboration_ in shared workspaces should also exchange information on the underlying digital sources and tools [@dourish1992]. To synchronize the current progress of others' work, user interfaces should carefully shift users' awareness by visually emphasizing changes made by others, a concept they call _shared feedback_.

It wasn't until the next century, however, that such collaboration could be realized on the web. Initially, the web platform relied solely on HTTP for transmitting data between clients and servers. HTTP could indeed be _abused_ for this [@fette2011, p. 4], which resulted in an unwanted overhead due to the ephemerality of the protocol: HTTP-issued TCP connections are usually closed after the request-response exchange. Shared workspaces, however, as emphasized by \citeauthor{dourish1992}, demand bidirectional communication. The web platform meanwhile specified two such bidirectional, real-time communication protocols that are commonly supported in today's web browsers: the WebSocket protocol and WebRTC.

The WebSocket protocol was designed as a simple solution to the aforementioned lack of continuous, bidirectional communication on the web. By using existing HTTP connections established via TCP, and subsequently _upgrading_ them to persistent WebSocket connections, both server and client can transmit data to either side [@fette2011, p. 5ff]. The Web Real-Time Communication protocol (WebRTC) continues to establish persistent and secure connections not just between client and server, but rather as a peer-to-peer (P2P) protocol between all kinds of devices [@w3c-webrtc]. WebRTC requires a handshaking mechanism via a trusted third party---a signaling service---in order to exchange connection information called Interactive Connectivity Establishment (ICE). To mitigate common issues of personal devices connected via Network Address Translation (NAT) or firewalls, WebRTC relies on further technology such as STUN and TURN.

The widespread support for such technologies in current web browsers sparked a wave of web-based applications that offer spaces for real-time collaborative work. In the following, I will name some popular contestants of such tools for remote collaborative work:

* Google Docs[^google-docs], an online, collaborative word processor with elaborate support for annotation and editing.
* Trello[^trello], a collaborative project management tool that allows to assign and move tasks across different boards (i.e., domains).
* Figma[^figma], a vector-based design tool for creating and testing user interface designs. Figma allows collaboration by synchronizing the canvas with participants in real-time, as well as their cursor movements.
* Notion[^notion], an online knowledge management tool. Similar to a Wiki, Notion allows to create pages composed by building blocks such as text and multimedia. Notion shows real-time activity of other users by synchronizing as their focus moves between each such block.

[^figma]: <https://figma.com/>.
[^google-docs]: <https://docs.google.com/>.
[^trello]: <https://trello.com/>.
[^git]: <https://www.git-scm.com/>.
[^notion]: <https://notion.so/>.

## Linked Data and the Digital Humanities {#sec:related:ld-dh}

Digital Humanities---an inherently digital discipline---concerns various interdisciplinary fields of research such as digital archeology, digital history, and digital classics. Hence, the landscape of tools and infrastructure in Digital Humanities research is vast and yet lacks a coherent overview. Initiatives such as the EU-funded DARIAH[^dariah] attempt to improve literacy with digital tools and provide a dedicated infrastructure available to a pan-European audience of research groups.

\citeauthor{hunyadi2016} notes that due to its interdisciplinarity, communities in the Digital Humanities frequently depend on remote collaborate. Using software for real-time video conferencing, researchers quickly established video-based plenary talks and virtual conferences [@hunyadi2016]. Scholarly collaboration continues with shared digital infrastructures among labs and projects. By embracing best-practices guidelines on research data management such as the FAIR principles[^fair-principles], initiatives promote sustinable workflows for handling digital assets among scholars: Findability, Accessibility, Interoperability, and Reusability of research data all shall ensure that data measured, processed, and evaluated in research is properly managed [@wilkinson2016].

Linked Open Data (LOD) could meet such requirements. While HTML provides elements for semantically tagging content, such  as titles, descriptions, and marginal notes, it lacked ways of describing _pure_ data without markup. Linked Data (LD) and, more generally, the Semantic Web, attempts to bring a semantic framework to data. The LD stack is build upon established web technologies, such as HTTP and URLs, and imposes semantic relations via triples modeled after the Resource Description Framework (RDF). With RDF triples, an external party can express the semantic relation between two entities by specifying a _subject_, a _predicate_, and an _object_[^rdf-example] [@bizer2009]. Linked Open Data continues even further and makes publicly accessible Linked Data.

With the Solid project[^solid], a group led by Berners-Lee conceived an architecture for managing personal data separately from applications and services on the web with online storage services called _pubs_ [@mansour2016]. Solid introduces Linked Data Platforms (LDPs), which are services that exclusively manage Linked Data containers---i.e., semantic collections of LD items---and their related media assets. Furthermore, LDPs specify how clients can interact with the stored data via REST-based APIs[^rest-api].

LOD can play a crucial role on the realization of FAIR in regard to data repositories in the Digital Humanities. Since LOD is publicly accessible, researchers can prepapre collections of data accordingly and publish them via HTTP, each with their own URI for distinctly referencing them on the web. Shared online gazetters, for example, provide collections of LD-formatted places and can be semantically onto other resources, such as ancient maps and classic texts [@simon2015].

Listing: An example annotation in form of a JSON-LD-based Web Annotation, as pictured by the Web Annotation data model technical report [@web-anno-data-model]. This annotation adds a textual annotation containing the text _j'adore !_ to a web resource.

```{#lst:web-annotation-model .json}
{
  "@context": "http://www.w3.org/ns/anno.jsonld",
  "id": "http://example.org/anno5",
  "type": "Annotation",
  "body": {
    "type" : "TextualBody",
    "value" : "<p>j'adore !</p>",
    "format" : "text/html",
    "language" : "fr"
  },
  "target": "http://example.org/photo1"
}
```

The Web Annotation specification emerged from the persistent lack of standardized annotation on the web and builds upon the previously defined concepts of LOD and LDPs [@sanderson2013]. The specification consists of two components: First, the Web Annotation Data Model, in which annotations are expressed using the JavaScript Object Notation (JSON), or more precisely the JSON-LD schema for use in LD environments. Following a versatile ontology similar to the observations from \citeauthor{marshall1997}, an annotation fundamentally consists of three properties, as pictured in @lst:web-annotation-model:

* The annotation's _identifier_, which is a web resource specified via its Internationalized Uniform Identifier (IRI, similar to an URI).
* Its _target_, which is a web resource, also specified via its IRI.
* An annotation _body_, which again can be a web resource. Alternatively, as done in @lst:web-annotation-model, the body can be provided as an inlined JSON object. For expressing a specific portion of the target resource---e.g., a piece of text, or a section of an image---a wide range of standardized _fragment selectors_ can be used.

The Web Annotation Protocol, as the second component, defines the Application Programming Interface (API) of an annotation server and thus, how client applications can transmit Web Annotations via HTTP. The API identifies annotations by their IRI[^annotation-iri] and consists of four basic verbs known from REST-based APIs: Clients can retrieve annotations (`GET`), create annotations (`POST`), update annotations (`PUT`), and delete annotations (`DELETE`). These actions can be executed on collections of annotations---semantic groups of items called _containers_ on the LDP---or the respective annotation, referred to by each their respective IRI.

With dokieli[^dokieli], \citeauthor{capadisli2017} created a publishing environment that supports storing documents on loosely-connected datastores---i.e., documents created on dokieli are not necessarily tied to it. By adhering to a multitude of LD protocols and supporting the LDP, dokieli is compliant with the aforementioned Solid architecture and attempts to support a separation between datastores and the respective applications operating on these stores. Furthermore, dokieli provides social interactions such as _liking_ sections of text and commenting by leveraging the Web Annotation protocol and data model.

![Annotating an ancient map on Recogito. Recogito allows to annotate sources, establish semantic relations---e.g., referencing places or people---and share these collections with other users.](figures/recogito-annotation.png){#fig:recogito-annotation short-caption="Annotating an ancient map on Recogito"}

The capabilities of Web Annotation are not limited to social interactions on text. Initiated by the Pelagios project[^pelagios], Recogito is a platform that has been created purposely for _semantic_ annotation [@simon2015; @simon2017]. In order to provide semantic annotation, Recogito leverages LD principles and allows to semantically tag portions of a source with LD collections, such as the aforementioned gazetteers or historic individuals. Besides text, Recogito also supports annotating static images and canonical resources, such as documents identified by CTS URNs or large-scale image galleries via International Image Interoperability Framework (IIIF).

[^dariah]: <https://www.dariah.eu/>.
[^fair-principles]: <https://www.go-fair.org/fair-principles/>.
[^rdf-example]: In RDF, objects are identified by their URIs. In an example given by \citeauthor{bizer2009}, an subject (http://dig.csail.mit.edu/data#DIG) relates via a predicate (http://xmlns.com/foaf/0.1/member) to an object (http://www.w3.org/People/Berners-Lee/card#i).
[^solid]: Although started by Berners-Lee at MIT, the Solid project is now managed by an affiliated commercial startup called Inrupt: <https://solidproject.org/>.
[^rest-api]: API is an acronym for Application Programming Interface. With APIs that adhere to Representional State Transfer (REST), applications can execute common actions such as creating, retrieving, and editing data via HTTP.
[^annotation-iri]: An example IRI would be: https://www.example.com/container-name/annotation-id.
[^dokieli]: <https://dokie.li/>.
[^pelagios]: <https://pelagios.org/>.

## Peer-to-Peer Networks {#sec:related:p2p}

The earliest concepts of hypertext established client-server architectures for storing hypertext documents distributed among an array of hypertext servers [@nelson1993; @berners-lee1989a]. These architectures, as opposed to a monolithic structure, allow the web to be decentralized and collaborative. Yet data on the web tends to gather in silos on a minority of servers, threatening the decentralized nature of the web [@capadisli2017]. Federated systems significantly emphasize decentralization by ensuring interoperability between multiple servers participating in the same network and thus composing one distributed system.

\citeauthor{esguerra2011} surveys such federated social networks, motivated by issues on privacy and censorship of large-scale social networks such as Facebook and Twitter: "Federated social networks [...] are a vital step towards fulfilling values often lacking in the existing social networking ecosystem: user-control, diversity of services, innovation, and more" [@esguerra2011]. The federated social network Diaspora[^diaspora] has been an early attempt at building such a federated social network. Building on the Diaspora network's success, Mastodon[^mastodon] became increasingly popular in recent months with about 3.9 Million users across about 2.600 instances as of today[^fediverse-network]. By implementing the standardized ActivityPub protocol [@activitypub], applications such as Mastodon leverage semantic LD principles and advance interoperability even further. Similar to the LDP, ActivityPub specifies server-to-server interactions with semantically tagged data.

![Architectures of communication networks [@baran1964]. In a centralized network, a single node has authority over all other nodes. Decentralized systems introduce supernodes as opposed to a single authority. Distributed or P2P networks diminish the notion of supernodes and introduce a fully distributed governance.](figures/network-architectures.png){#fig:related:architectures short-caption="Architectures of communication networks"}

@Fig:related:architectures pictures three different layouts for communication networks with each node of the network connected to others by distinctive strategies [@baran1964]. Centralized networks, as shown on the leftmost diagram, promote a single node to govern all other nodes which can render efficient systems, albeit bearing a single point of failure due to singular authority. Decentralized networks divide centralized power into a multitude of nodes, commonly called _supernodes_. The web, as mentioned above, has been designed with a decentralized architecture where web servers function as supernodes. Instead of connecting to a single node for all their needs, clients on the web connect to a variety of servers via their Internet Protocol (IP) addresses. Federated systems emphasize server-to-server communication for interoperability, pictured as connections between supernodes in the center diagram of @fig:related:architectures.

In distributed networks, shown on the rightmost diagram, resources are distributed equally among peers. However, fully decentralizing power of a complex system can pose substantial challenges: How can nodes on the network determine where they may request a particular piece of data from? How can nodes verify that the data they received has not been manipulated by a malicious actor? How can a node prevent others from changing a read-only piece of data? The technologies I will introduce in the following attempt to address some of these challenges.

Distributed Hash Tables (DHTs) attempt to solve the issue of discoverability and can be propagated _within_ a distributed network or managed externally as an _overlay network_. A DHT stores connection information for nodes in a hash table that is distributed among nodes---as opposed to a more centralized solution like Domain Name System (DNS) commonly used on the web. Since the hash table is distributed, nodes may need multiple hops---i.e., connecting to a multitude of nodes for obtaining the required parts of the DHT---in order to resolve the data of a particular node [@maymounkov2002]. The amount of hops needed for retrieving information on particular nodes defines the distance between nodes and thus the efficieny of a DHT.

![Binary tree of a Kademlia DHT [@maymounkov2002]. Nodes are sorted within the tree by each their ID, a 160-bit number. In order to discover other nodes in other subtrees of the DHT, a node (black dot with ID `0011...`) needs to have contact with nodes of the other subtreets (gray ovals).](figures/kademlia-tree.png){#fig:kademlia-tree short-caption="Binary tree of a Kademlia DHT"}

One such DHT is Kademlia [@maymounkov2002], pictured in @fig:kademlia-tree. A Kademlia-type DHT manages nodes of a network in a binary tree, sorted by their respective ID, which is a 160-bit number. These binary trees are divided into subtrees, and each subtree's nodes maintain connection information on each other. With this binary subtree separation, the Kademlia DHT defines the distance between peers as an an XOR metric based on their ID, i.e., the more different two nodes IDs are in terms of their bit representation, the more distant they become. For use in BitTorrent, \citeauthor{loewenstern2020} developed a DHT based on Kademlia that contains extensions related to torrents---i.e., distributed archives on BitTorrent---and the BitTorrent protocol [@loewenstern2020].

An append-only log is a list-based data structure that exclusively allows the addition of entries: "A log is perhaps the simplest possible storage abstraction. It is an append-only, totally-ordered sequence of records ordered by time" [@kreps2013]. \citeauthor{nelson1993} previously described such a log for managing document histories on Xanadu [@nelson1993, 2/15]. Popularized by stream processing frameworks such as Apache Kafka[^apache-kafka] and Apache Samza[^apache-samza], as well as by collaborative systems such as the Git[^git] version control system, append-only logs treat the current state of a database as a chronological sequence of changes rather than a definite state. Append-only logs provide a particularly interesting prospect for their distribution, as malicious actors are not able to simply modify the history of a log. Furthermore, entries of a log reference to each other via their content---i.e., content-addressing via hashes---and thus, if an actor of a distributed system mutates an existing entry, all following entries' hashes will change subsequently and the log will break.

Such technologies are commonly utilized in file-sharing systems, with both Gnutella [@chawathe2003] and BitTorrent [@legout2007] being well-known contendants that have been frequently researched. Such systems emerged during the popularity of P2P applications in the early 2000s and were commonly used for sharing copyrighted content despite---or, possibly because of---their technological efficiency [@gearlog2010]. Contemporary approaches rather emphasize the development of decentralized applications for bypassing censorship, enabling end-to-end encryption, or facilitating real-time collaboration in day-to-day usecases. IPFS builds on top of the `libp2p`[^libp2p] library and creates a world-wide, distributed, and secure filesystem based on content-addressing [@benet2014]. Dat is a data-sharing protocol actively developed by the Dat Protocol foundation for use in civic and research technology [@robinson2018]. By leveraging a modularized technology stack implemented in JavaScript---such as the Hypercore[^hypercore] append-only log and the Hyperswarm[^hyperswarm] networking stack---Dat can be used in decentralized applications on the web and on the desktop alike. Secure Scuttlebutt (SSB) is a decentralized social network that uses a _gossip_ protocol under the assumption that by continuously broadcasting messages on the network, all peers will eventually obtain a consistent state [@tarr2019].

[^apache-kafka]: <https://kafka.apache.org/>.
[^apache-samza]: <https://samza.apache.org/>.
[^git]: <https://git-scm.com/>.
[^diaspora]: <https://diasporafoundation.org/>.
[^mastodon]: <https://joinmastodon.org/>.
[^fediverse-network]: The [fediverse.network](https://fediverse.network/) website provides various usage and network statistics on such as Mastodon, PeerTube, and WordPress: <https://fediverse.network/mastodon>. The statistics mentioned on Mastodon were current as of April 8, 2020.
[^libp2p]: `libp2p` provides a modular stack for building decentralized applications: <https://libp2p.io/>.
[^hypercore]: <https://github.com/mafintosh/hypercore>.
[^hyperswarm]: <https://github.com/hyperswarm/hyperswarm>.

## Local-First Applications {#sec:related:local-first}

In \citeyear{kleppmann2019}, \citeauthor{kleppmann2019} coined the term _local-first applications_. In a paper called \citetitle{kleppmann2019}, the authors propose a set of requirements for a novel type of application that overcomes common issues of contemporary real-time collaboration software [@kleppmann2019]: As businesses increasingly rely on cloud providers to manage and provision their technical infrastructure, fewer applications continue to work when not connected to the internet or---even more severe---the company ceases to support the product. Most profoundly, local-first applications expect _all_ respective data to be stored locally, which could be text documents or structured databases. By cherishing local data, operations on these datastores happen instantaneously and network connections become optional. Real-time updates are being transmitted nevertheless once back online and are continuously reflected on the user interface.

Issues commonly arise when two users change the same property on a piece of data: If two people were to edit a paragraph of text collaboratively in the same shared document and edited the same word at the same time before synchronizing, this situation would cause a conflict. A centralized authority can occasionally solve such conflicts by applying particular sets of rules for conflict resolution. However, even authorative entities---e.g., backend servers of some of the collaborative products listed above in @sec:related:collaboration---frequently fail at automatic merging, resulting in manual conflict resolution and potential data loss for users [@kleppmann2019].

For use in such environments with frequent changes and conflicts as well as no authorative merging entity, Conflict-free Replicated Data Types (CRDTs) impose a set of strategies on how to merge various data structures, such as grow-only sets and increment-only integer counters. Extensively documented by \citeauthor{shapiro2011}, CRDTs utilize the assumption of Strong Eventual Consistency (SEC) to guarantee that after all peers exchange their operations and individually apply the same merging strategies on each their state, eventually all individual data converges to the same state [@shapiro2011]. Due to their flexibility, CRDTs can be applied to a variety of circumstances, such as distributed collaborative systems. Thus, as CRDTs gained popularity among developers for building applications with offline capabilities and non-authorative merging, implementations for various platforms and programming environments emerged in research and development communities[^crdt-website]. \citeauthor{kleppmann2017} transferred the capabilities of CRDTs to JSON data structures, as JSON objects consist---besides of atomic types---of maps and lists, both of which can be commonly merged using CRDT strategies [@kleppmann2017]. With Automerge[^automerge], they have released a JavaScript-based CRDT that builds upon this research and runs in today's web browsers [@kleppmann2018].

Building upon Automerge, \citeauthor{kleppmann2019} created a library called _Hypermerge_[^hypermerge] that integrates conflict-free merging with some of the aforementioned contemporary P2P approaches. Hypermerge documents are identified by their URL (e.g., `hypermerge:/abc123`), which resolves to the underlying distributed data structure. Hypermerge leverages the Dat project's Hypercore append-only log, which enables to storing Automerge changes in chronological order as they occur over time and provides a streaming-based replication mechanism. Hypercore provides additional support exchanging ephemeral messages with peers related to the same log and uses the Noise protocol[^noise], a novel communication protocol with advanced cryptographic privacy features. The Hyperswarm decentralized network and DHT then allows peers to discover each other and subsequently replicate their Hypercore logs of the same document.

In a follow-up publication, \citeauthor{hardenberg2020} detail the implementation of a proof-of-concept, local-first application called _PushPin_ that leverages the Hypermerge library as foundation for the application’s data storage. This enables PushPin to operate when offline, yet providing real-time collaboration when online and sharing workspaces with others. Several take-aways emerged from their work, including the benefits of Functional Reactive Programming (FRP) for user interfaces displaying frequent real-time updates as well as ongoing issues around privacy, security, and usability of local-first applications [@hardenberg2020].

[^crdt-website]: The [crdt.tech](https://crdt.tech/) website curates lists of various CRDT implementations complemented by related research papers and a brief documentation around CRDTs: <https://crdt.tech/>.
[^automerge]: <https://github.com/automerge/>.
[^hypermerge]: <https://github.com/automerge/hypermerge>.
[^noise-protocol]: <https://noiseprotocol.org/>


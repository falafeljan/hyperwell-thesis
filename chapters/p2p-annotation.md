# Peer-to-Peer Annotation {#sec:annotation}

> _Although they let you access your data anywhere, all data access must go via the server, and you can only do the things that the server will let you do. In a sense, you don’t have full ownership of that data --- the cloud provider does. In the words of a bumper sticker: “There is no cloud, it’s just someone else’s computer.”_ [@kleppmann2019, p. 155]

Despite its decentralized nature, resources on the web tend to be centralized on a small number of servers. This development intensified even more amidst the rise of Big Data and the continuous commiditization of personal data, threatening data ownership and autonomy on the web. Arguably, academia and particularly the Digital Humanities treat the web differently by embracing LD principles and establishing sustainable and self-organized infrastructures. Furthermore, with the Web Annotation specification issued by the W3C, academic annotation services can be provided interoperably by research environments and digital libraries. Considering annotation in the overall status quo, however, poses the following questions: Does annotation belong to the institutional domain? If not, where else other than the web to store and distribute them?

In the following, I will argue for establishing an architectural separation between personal annotation and the respective sources based on the assumption that annotations are social data. @Sec:annotation:web revisits characteristics of web; questioning implications of cloud infrastructure in today's internet services, I discuss effects of decentralization in federated networks and P2P systems on digital annotation. P2P systems commonly rely on high-availability infrastructures known as _supernodes_ or _mirrors_ for replication and indexing. @Sec:annotation:infrastructure examines these infrastructures and draws inspiration from public services such as libraries.

In @sec:annotation:ownership, I then define the distinctive terms of _notebooks_ and _public institutions_ for establishing a separation between personal data and public services in P2P networks and the web. Subsequently, in the following chapter~\ref{sec:implementation}, I discuss proof-of-concept designs and implementations on how such a separation can be realized with P2P technology.

## What's (Not) Wrong with Servers? {#sec:annotation:web}

The web establishes a decentralized network---as reported previously in this thesis---since hypertext systems commonly use a client-server model in order to distribute hypertext documents among a multitude of servers on the network [@berners-lee1989a; @nelson1993]. This architectural decision brings its pros and cons; smaller networks can benefit from this model by being easily scalable and distributing resources predictably, yet large networks of world-wide scale can quickly outgrow their intention of being open, accessible, and collaborative, threatening their heterogeneity:

> _While the Web has the potential to enable full open access to knowledge, the code that powers the Web is not built for that. Instead, the Web uses a centralized data model optimized for use by commercial organizations. In other words, today’s Web values the access and voices of people who are valuable to corporate interests._ [@robinson2018, p. 2]

Businesses became increasingly obsessed with harvesting personal data, which provides them with questionable[^cambridge-analytica][^shadow-profiles] substance for training targeted advertising models and personalization in news feeds [@bucher2012; @robinson2018]. The commodification of personal data by businesses consequently spoon-fed the progression of Big Data and the partitioning of the web into monolithic data silos [@srnicek2017]. Considering the resulting redistribution of resources on the web as a whole, this unequality led to a fundamental break with the web's decentralized nature:

> _While the Web was originally conceived as a decentralised platform where every organisation and individual can participate, it became increasingly centralised with less than 1% of the servers serving more than 99% of the content._ [@capadisli2017, p. 469]

Arguably, LDP-supporting environments such as Solid and dokieli attempt to counteract such developments by emphasizing interoperability via server-to-server communication and open standards for data exchange [@mansour2016; @capadisli2017]. Based on foundational web technologies, decentralized protocols like ActivityPub showed that the web can serve as a topsoil for autonomous and sustainable communities. Furthermore, publicly shared and semantically enriched data repositories can contribute to the digital commons; in Digital Humanities research, such contributions could arise as digital commentary via Web Annotation and digital gazetteers [@simon2017].

In this context, a published collection of annotations on a resource could be considered a critical commentary and thus make for a work of its own [@marshall1997; @marshall1998]. Marginal notes often provide context for sophisticated discussion among readers, rendering such annotations social data. Furthermore, as \citeauthor{marshall1998} notes in her \citeyear{marshall1998} analysis of annotations, there exists a significant difference between annotations that have been made for private purposes and those which were created with public discussion in mind: Similar to a private diary, private annotations are created for private use, and thus personal ownership as well privacy should be protected.

With _local-first software_, \citeauthor{kleppmann2019} introduces a novel type of application based on established concepts with the aim of retaining personal ownership of data [@kleppmann2019]. One technology that is particularly well-suited for use in local-first applications is a CRDT, a data structure that provides strategies for conflict-free merging of changes emerging from a distributed system without a centralized authority. Under the theoretical assumption of Strong Eventual Consistency (SEC), the application of each separately obtained log of changes will eventually result in the same state on all peers of the system. By furthermore urging for data interoperability and encryption, \citeauthor{kleppmann2019} make local-first apps a promising contendant for fixing the web's ownership issues.

Interestingly, the ecosystem of the web is changing. Rooted in the hype for P2P networks from the early 2000s, several protocols for hypermedia data-sharing emerged; namely Dat[^dat-foundation] and IPFS[^ipfs], both of which attempt to provide a foundation for redistributing resources on the web. Yet, some of their prospects are not fundamentally new and could actually contribute to transforming the web towards earlier notions of hypertext systems, like Xanadu [@voss2019]. Today's browsers increasingly adopt these maturing protocols, with support for Dat and IPFS in Beaker[^beaker] and Opera[^opera-ipfs], respectively.

However, novel systems which build upon these protocols will need to ensure interoperability with the current web: On the one hand, it provides a vast amount of knowledge, e.g., publicly accessible PDF documents or semantically-tagged LOD repositories from DH research. On the other hand, the web provides an efficient way for public entities, such as academic institutions and digital libraries, to publish content under their authority. In the following, I attempt to discuss the role of such entities in P2P systems.

[^cambridge-analytica]: <https://www.theguardian.com/technology/2019/jul/12/facebook-fine-ftc-privacy-violations>.
[^shadow-profiles]: <https://www.vox.com/2018/4/20/17254312/facebook-shadow-profiles-data-collection-non-users-mark-zuckerberg>.
[^dat-foundation]: <https://dat.foundation/>.
[^ipfs]: <https://ipfs.io/>.
[^beaker]: <https://beakerbrowser.com/>.
[^opera-ipfs]: The Opera browser for Android recently introduced support for resolving IPFS URLs: <https://blog.ipfs.io/2020-03-30-ipfs-in-opera-for-android/>.

## Public Entities in Peer-to-Peer Systems {#sec:annotation:infrastructure}

_Supernodes, relays, pubs, pods, proxies, gateways, pinning, seeding, mirroring:_ Many popular decentralized networks leverage infrastructures that provide resources for sustaining the networks’ operation. For establishing a stable and reliable overlay network, nodes can be promoted to _supernodes_ based their available resources, such as computational power, uptime, and bandwidth [@guha2005]. This can be beneficial for real-time communication systems with high demands on bandwidth. Systems that provide file-sharing capabilities, replicating archives and indexing of data becomes highly relevant as; since personal devices commonly join networks on-demand and establish temporary connections, their data vanishes as they go offline. Many of the systems discussed in @sec:related:p2p leverage 

In the following, I will use some terms interchangeably. While peers concern the abstract notion of homogeneous participants in a system, nodes are connoted rather technical. Supernodes commonly _relay_ or _tunnel_ data to a multitude of nodes. In file-sharing systems, data can be _replicated_ or _mirrored_, as multiple nodes provide a copy of a resource that is indexed under the same address, e.g., in a DHT. I call these kinds of services _public_: They often are of a _volatile_ or _ephemeral_ nature, but help to maintain a complex, distributed system.

Distributed P2P systems function fundamentally different from established architectures that separate between clients and servers in a network. The fundamental difference is explained by how the participants treat data: In architectures following the established client-server separation, such as HTTP, servers hold a monopoly of the contained data while clients request parts of this data on-demand. This provides several benefits for businesses: They are able to govern the singular source of their services’ data by properly "owning" it. This means, businesses are effectively controlling aspects such as data availability, access to data, its versioning, and basically any kind of operation on it, ensuring commercial exploitation. (Something on providing guaranteed uptime, data backups, etc.).

In P2P systems, this power over data is distributed. The distinction of clients and servers is being blurred as the centralization of governance is diminished: Clients become servers, forming a collection of alike peers, that provide and at the same time request data. Considering “the data” a system operates on as a database (with support for querying and mutation), in these kind of distributed systems, this database is distributed, sometimes even fragmented.

This poses many questions when conceiving P2P architectures: Which parts do work well centralized? Which functionality does effectively when being distributed? How can certain control structures be realized?

One trade-off of theoretically "pure" P2P systems is, considering all data is exchanged between genuine peers, that each peer is running on commodity hardware—regular consumer devices. Especially in these days, where an increasing number of our interactions with the digital world occurs via handheld devices such as smartphones, their lack of processing power compared to the enormous computational resources of a dedicated cluster is troublesome. Yet, with the wake of the more mature, "smarter" P2P systems, these inequalities were to be addressed. Skype, for instance, as research by @guha2005 showed, analyzed peers’ network performance and promoted particular peers to supernodes. These supernodes “maintain an overlay network network among themselves” [@guha2005] and effectively outbalance the weaknesses of less powerful peers [@chawathe2003].

## Distributing Ownership {#sec:annotation:ownership}

Finally, come to the conclusion that personal data can _in fact_ be already stored on and exchanged with P2P networks. Make it concern digital libraries as manifestation of well-indexed, static sources.

Analogies with libraries and personal notes [@wilensky2000].

![Separation of annotations as personal data and resources as institutional data.](figures/ownership-separation.pdf){#fig:separation short-caption="Separation of personal and institutional data"}

_TODO:_ Make a stretch towards the distinction of _personal_ and _institutional_ data. Thus, impose the notions of _notebooks_ (personal collections of annotations) and _institutional services_ (such as archiving, mirroring, and gateways) or _collections_.

Notebooks are collections of personal annotations on a particular source. These notebooks should be considered private and thus their initial personal ownership should be conserved, for example, by cryptographic means. Furthermore, notebooks should be kept private even when sharing them with selected peers, either for display or active collaboration, and encryption should be leveraged to keep them private from third parties. They should be explicitely versioned to allow for time traveling, and should enforce an explicit open data model for ensuring interoperability with a variety of applications and systems.  

Libraries are akin to this notion of institutions; they provide a (majorly) static collection of resources that are properly indexed and accessible via canonical identifiers. Examples for such institutions are the Perseus Digital Library[^perseus-dl] and the British Library[^british-library].

[^perseus-dl]: The Perseus Digital Library provides a large amount of classic texts online [@smith2000]. The library’s Scaife Viewer allows for browsing its growing collection of texts available via CTS: <https://scaife.perseus.org/>.
[^british-library]: The British Library uses the IIIF standard for offering some of their digitized items: <https://www.bl.uk/subjects>.
